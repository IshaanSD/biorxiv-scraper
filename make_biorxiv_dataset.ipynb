{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\").tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/johngiorgi/Desktop/biorxiv_dump\"\n",
    "output_dir = \"/Users/johngiorgi/Documents/dev/t2t/datasets/biorxiv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(input_dir)\n",
    "scraped_content = {}\n",
    "# This will drop ~8% of all articles in bioRxiv\n",
    "min_word_count = 25\n",
    "\n",
    "for path in input_dir.iterdir():\n",
    "    if not path.name.endswith(\".pickle\"):\n",
    "        continue\n",
    "    with open(path, \"rb\") as f:\n",
    "        scraped_content.update(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 59178/59598 (99.30%) of articles after filtering for a length of 25 tokens.\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "scraped_content_cleaned = {}\n",
    "\n",
    "for doi, content in scraped_content.items():\n",
    "    # Remove whitespace, newlines and tabs\n",
    "    abstract = ' '.join(content['abstract'].split())\n",
    "\n",
    "    # This will drop ~8% of all articles in bioRxiv\n",
    "    if len(tokenizer(content[\"abstract\"])) < min_word_count:\n",
    "        continue\n",
    "        \n",
    "    scraped_content_cleaned[doi] = content\n",
    "    x.append(abstract)\n",
    "    y.append(content[\"subject_area\"])\n",
    "    \n",
    "    \n",
    "print(f\"Retained {len(x)}/{len(scraped_content)} ({len(x)/len(scraped_content):.2%}) of articles after filtering for a length of {min_word_count} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.80\n",
    "valid_size = 0.10\n",
    "test_size = 0.10\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y,\n",
    "    test_size=1-train_size,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    ")\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(\n",
    "    x_test, y_test, test_size=test_size/(test_size + valid_size),\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train)\n",
    "assert len(x_valid) == len(y_valid)\n",
    "assert len(x_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(output_dir)\n",
    "\n",
    "with open(output_dir / \"train.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(x_train))\n",
    "with open(output_dir / \"valid.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(x_valid))\n",
    "with open(output_dir / \"test.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(x_test))\n",
    "with open(output_dir / \"train_labels.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(y_train))\n",
    "with open(output_dir / \"valid_labels.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(y_valid))\n",
    "with open(output_dir / \"test_labels.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(input_dir) / \"scraped_content_clean.pickle\", \"wb\") as f:\n",
    "    pickle.dump(scraped_content_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59178"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ab for ab in x if not ab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "embeddings = []\n",
    "with open(\"/Users/johngiorgi/Documents/dev/t2t/datasets/biorxiv/embeddings.jsonl\", \"r\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        content = json.loads(line)\n",
    "        embeddings.append(content[\"doc_embeddings\"])\n",
    "assert len(scraped_content_cleaned) == len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doi, embedding in zip(scraped_content_cleaned, embeddings):\n",
    "    scraped_content_cleaned[doi][\"w2v\"] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(input_dir) / \"scraped_content_clean.pickle\", \"wb\") as f:\n",
    "    pickle.dump(scraped_content_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:t2t]",
   "language": "python",
   "name": "conda-env-t2t-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
