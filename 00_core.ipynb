{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "from tqdm import trange\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class bioRxivScraper():\n",
    "    \"\"\"A simple class for scraping articles and their metadata from bioRxiv.\"\"\"\n",
    "    def __init__(self):\n",
    "        url_advanced_search_base = \"https://www.biorxiv.org/search/%20jcode%3Abiorxiv\"\n",
    "        # F-formatted string for advanced search querys\n",
    "        url_advanced_search_params = (\"%20subject_collection_code%3A{0}%20limit_from%3A{1}-{2}-\"\n",
    "                                      \"01%20limit_to%3A{1}-{3}-01%20numresults%3A100%20sort%3Arelevance\"\n",
    "                                      \"-rank%20format_result%3Astandard\")\n",
    "\n",
    "        self.url_advanced_search = url_advanced_search_base + url_advanced_search_params\n",
    "        self.url_direct_link_base = 'https://www.biorxiv.org/{0}'\n",
    "        \n",
    "    def by_subject_area(self, subject_area: str, start_year: int, end_year: int = None) -> Dict:\n",
    "        \"\"\"Returns a dictionary keyed by doi, that contains the data/metadata of all articles uploaded \n",
    "        between `start_year` and `end_year` for a given `subject_area`.\n",
    "        \n",
    "        Args:\n",
    "            subject_area (str): \n",
    "        \"\"\"\n",
    "        end_year = start_year if end_year is None else end_year\n",
    "        # Format the subject area string such that it can be encoded in a URL\n",
    "        url_encoded_subject_area = '%20'.join(subject_area.split(' '))\n",
    "\n",
    "        scraped_content = {}\n",
    "\n",
    "        for year in trange(start_year, end_year + 1):\n",
    "            for month in trange(1, 2):\n",
    "                # Populate the fields with our current query and post it\n",
    "                resp = requests.post(\n",
    "                    self.url_advanced_search.format(url_encoded_subject_area, year, month, month + 1)\n",
    "                )\n",
    "                html = bs(resp.text)\n",
    "                # Collect the articles in the result in a list\n",
    "                articles = html.find_all('li', attrs={'class': 'search-result'})\n",
    "                for article in articles:\n",
    "                    # Pull the title, if it's empty then skip it\n",
    "                    linked_title =  article.find('a', attrs={'class': 'highwire-cite-linked-title'})\n",
    "                    content_id = linked_title['href'].strip()\n",
    "                    doi = '/'.join(content_id.split('/')[-2:])[:-2]\n",
    "                    title = linked_title.find('span', attrs={'class': 'highwire-cite-title'})\n",
    "                    authors = article.find_all('span', attrs={'class': 'highwire-citation-author'})\n",
    "\n",
    "                    if title is None:\n",
    "                        continue\n",
    "                    title = title.text.strip()\n",
    "\n",
    "                    resp = requests.post(self.url_direct_link_base.format(content_id))\n",
    "                    html = bs(resp.text)\n",
    "\n",
    "                    abstract = html.find(\"p\", attrs={\"id\": re.compile(r\"p-\\d+\")})\n",
    "\n",
    "                    if abstract is None:\n",
    "                        continue\n",
    "                    abstract = abstract.text.strip()\n",
    "\n",
    "                    # Collect year / month / title information\n",
    "                    # TODO (John): Can we scrape the published date, and add only\n",
    "                    # one field here: \"data_published\"\n",
    "                    scraped_content[doi] = {\n",
    "                        'month': month, \n",
    "                        'year': year, \n",
    "                        'title': title, \n",
    "                        'abstract': abstract,\n",
    "                        'authors': [author.text for author in authors],\n",
    "                    }\n",
    "\n",
    "        return scraped_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this cell in order to convert cells marked for export to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bioarxiv-scraper] *",
   "language": "python",
   "name": "conda-env-bioarxiv-scraper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
